{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/ttsai/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>premise</th>\n",
       "      <th>hypothesis</th>\n",
       "      <th>lang_abv</th>\n",
       "      <th>language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>c6d58c3f69</td>\n",
       "      <td>Ø¨Ú©Ø³Ø Ú©ÛØ³ÛØ Ø±Ø§ÛÛÙØ ÛØ³Ø¹ÛØ§ÛØ...</td>\n",
       "      <td>Ú©ÛØ³Û Ú©Û ÙØ¦Û Ú©ÙØ¦Û ÛØ§Ø¯Ú¯Ø§Ø± ÙÛ...</td>\n",
       "      <td>ur</td>\n",
       "      <td>Urdu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cefcc82292</td>\n",
       "      <td>ÙØ°Ø§ ÙÙ ÙØ§ ØªÙ ÙØµØ­ÙØ§ Ø¨Ù.</td>\n",
       "      <td>Ø¹ÙØ¯ÙØ§ ÙØªÙ Ø¥Ø®Ø¨Ø§Ø±ÙÙ Ø¨ÙØ§ ÙØ¬Ø¨...</td>\n",
       "      <td>ar</td>\n",
       "      <td>Arabic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>e98005252c</td>\n",
       "      <td>et cela est en grande partie dÃ» au fait que l...</td>\n",
       "      <td>Les mÃ¨res se droguent.</td>\n",
       "      <td>fr</td>\n",
       "      <td>French</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>58518c10ba</td>\n",
       "      <td>ä¸åå¸åå¶ä»å¬æ°åç¤¾åºç»ç»ä»£è¡¨å...</td>\n",
       "      <td>IMAä¸å¶ä»ç»ç»åä½ï¼å ä¸ºå®ä»¬é½ä¾é...</td>\n",
       "      <td>zh</td>\n",
       "      <td>Chinese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>c32b0d16df</td>\n",
       "      <td>ÐÐ½Ð° Ð²ÑÐµ ÐµÑÐµ Ð±ÑÐ»Ð° ÑÐ°Ð¼.</td>\n",
       "      <td>ÐÑ Ð´ÑÐ¼Ð°Ð»Ð¸, ÑÑÐ¾ Ð¾Ð½Ð° ÑÑÐ»Ð°, Ð¾Ð...</td>\n",
       "      <td>ru</td>\n",
       "      <td>Russian</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id                                            premise  \\\n",
       "0  c6d58c3f69  Ø¨Ú©Ø³Ø Ú©ÛØ³ÛØ Ø±Ø§ÛÛÙØ ÛØ³Ø¹ÛØ§ÛØ...   \n",
       "1  cefcc82292             ÙØ°Ø§ ÙÙ Ù\n",
       "Ø§ ØªÙ\n",
       " ÙØµØ­ÙØ§ Ø¨Ù.   \n",
       "2  e98005252c  et cela est en grande partie dÃ» au fait que l...   \n",
       "3  58518c10ba  ä¸åå¸åå\n",
       "¶ä»å\n",
       "¬æ°åç¤¾åºç»ç»ä»£è¡¨å...   \n",
       "4  c32b0d16df              ÐÐ½Ð° Ð²ÑÐµ ÐµÑÐµ Ð±ÑÐ»Ð° ÑÐ°Ð¼.   \n",
       "\n",
       "                                          hypothesis lang_abv language  \n",
       "0  Ú©ÛØ³Û Ú©Û ÙØ¦Û Ú©ÙØ¦Û ÛØ§Ø¯Ú¯Ø§Ø± ÙÛ...       ur     Urdu  \n",
       "1  Ø¹ÙØ¯Ù\n",
       "Ø§ ÙØªÙ\n",
       " Ø¥Ø®Ø¨Ø§Ø±ÙÙ\n",
       " Ø¨Ù\n",
       "Ø§ ÙØ¬Ø¨...       ar   Arabic  \n",
       "2                            Les mÃ¨res se droguent.       fr   French  \n",
       "3  IMAä¸å\n",
       "¶ä»ç»ç»åä½ï¼å ä¸ºå®ä»¬é½ä¾é...       zh  Chinese  \n",
       "4  ÐÑ Ð´ÑÐ¼Ð°Ð»Ð¸, ÑÑÐ¾ Ð¾Ð½Ð° ÑÑÐ»Ð°, Ð¾Ð...       ru  Russian  "
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "train_df = pd.read_csv('train.csv', encoding='ISO-8859-1')\n",
    "test_df = pd.read_csv('test.csv', encoding='ISO-8859-1')\n",
    "\n",
    "clean_train_df = pd.read_csv('train_clean.csv', encoding='ISO-8859-1')\n",
    "clean_test_df = pd.read_csv('test_clean.csv', encoding='ISO-8859-1')\n",
    "\n",
    "clean_train_df['id'] = train_df['id']\n",
    "clean_test_df['id'] = test_df['id']\n",
    "\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ï»¿id</th>\n",
       "      <th>premise</th>\n",
       "      <th>hypothesis</th>\n",
       "      <th>lang_abv</th>\n",
       "      <th>language</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>c6d58c3f69</td>\n",
       "      <td>The boxes will be marked with the names of Buc...</td>\n",
       "      <td>There will be no memorial for Casey, Coleman H...</td>\n",
       "      <td>ur</td>\n",
       "      <td>Urdu</td>\n",
       "      <td>c6d58c3f69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cefcc82292</td>\n",
       "      <td>We were advised</td>\n",
       "      <td>When they are told what they should do, manage...</td>\n",
       "      <td>ar</td>\n",
       "      <td>Arabic</td>\n",
       "      <td>cefcc82292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>e98005252c</td>\n",
       "      <td>This is largely due to mothers taking drugs</td>\n",
       "      <td>Mothers are drugged</td>\n",
       "      <td>fr</td>\n",
       "      <td>French</td>\n",
       "      <td>e98005252c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>58518c10ba</td>\n",
       "      <td>Conversations with city and other civic and co...</td>\n",
       "      <td>IMA collaborates with other organizations as t...</td>\n",
       "      <td>zh</td>\n",
       "      <td>Chinese</td>\n",
       "      <td>58518c10ba</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>c32b0d16df</td>\n",
       "      <td>She</td>\n",
       "      <td>We thought she left but she stayed</td>\n",
       "      <td>ru</td>\n",
       "      <td>Russian</td>\n",
       "      <td>c32b0d16df</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5186</th>\n",
       "      <td>0242fd3023</td>\n",
       "      <td>He should probably tear up that paper and put ...</td>\n",
       "      <td>He will burn paper</td>\n",
       "      <td>th</td>\n",
       "      <td>Thai</td>\n",
       "      <td>0242fd3023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5187</th>\n",
       "      <td>c2e4b293bf</td>\n",
       "      <td>Al Qaeda and terrorism was one of the agendas ...</td>\n",
       "      <td>There were other important factors besides Al ...</td>\n",
       "      <td>sw</td>\n",
       "      <td>Swahili</td>\n",
       "      <td>c2e4b293bf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5189</th>\n",
       "      <td>f88b84f4e1</td>\n",
       "      <td>Very soon a friend of IRT will call to take yo...</td>\n",
       "      <td>You can donate money to the phone</td>\n",
       "      <td>ru</td>\n",
       "      <td>Russian</td>\n",
       "      <td>f88b84f4e1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5190</th>\n",
       "      <td>5f90dd59b0</td>\n",
       "      <td>Sleep promised that the motel had researched t...</td>\n",
       "      <td>Nemeth is being compensated for the motel inve...</td>\n",
       "      <td>ur</td>\n",
       "      <td>Urdu</td>\n",
       "      <td>5f90dd59b0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5192</th>\n",
       "      <td>1f0ea92118</td>\n",
       "      <td>Her current presence, and given the nature of ...</td>\n",
       "      <td>The fact that she was only present after the f...</td>\n",
       "      <td>zh</td>\n",
       "      <td>Chinese</td>\n",
       "      <td>1f0ea92118</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2250 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           ï»¿id                                            premise  \\\n",
       "0     c6d58c3f69  The boxes will be marked with the names of Buc...   \n",
       "1     cefcc82292                                    We were advised   \n",
       "2     e98005252c        This is largely due to mothers taking drugs   \n",
       "3     58518c10ba  Conversations with city and other civic and co...   \n",
       "4     c32b0d16df                                                She   \n",
       "...          ...                                                ...   \n",
       "5186  0242fd3023  He should probably tear up that paper and put ...   \n",
       "5187  c2e4b293bf  Al Qaeda and terrorism was one of the agendas ...   \n",
       "5189  f88b84f4e1  Very soon a friend of IRT will call to take yo...   \n",
       "5190  5f90dd59b0  Sleep promised that the motel had researched t...   \n",
       "5192  1f0ea92118  Her current presence, and given the nature of ...   \n",
       "\n",
       "                                             hypothesis lang_abv language  \\\n",
       "0     There will be no memorial for Casey, Coleman H...       ur     Urdu   \n",
       "1     When they are told what they should do, manage...       ar   Arabic   \n",
       "2                                   Mothers are drugged       fr   French   \n",
       "3     IMA collaborates with other organizations as t...       zh  Chinese   \n",
       "4                    We thought she left but she stayed       ru  Russian   \n",
       "...                                                 ...      ...      ...   \n",
       "5186                                 He will burn paper       th     Thai   \n",
       "5187  There were other important factors besides Al ...       sw  Swahili   \n",
       "5189                  You can donate money to the phone       ru  Russian   \n",
       "5190  Nemeth is being compensated for the motel inve...       ur     Urdu   \n",
       "5192  The fact that she was only present after the f...       zh  Chinese   \n",
       "\n",
       "              id  \n",
       "0     c6d58c3f69  \n",
       "1     cefcc82292  \n",
       "2     e98005252c  \n",
       "3     58518c10ba  \n",
       "4     c32b0d16df  \n",
       "...          ...  \n",
       "5186  0242fd3023  \n",
       "5187  c2e4b293bf  \n",
       "5189  f88b84f4e1  \n",
       "5190  5f90dd59b0  \n",
       "5192  1f0ea92118  \n",
       "\n",
       "[2250 rows x 6 columns]"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "only_eng_train = train_df.loc[train_df['language'] == 'English']\n",
    "only_eng_test = test_df.loc[test_df['language'] == 'English']\n",
    "without_eng_train = train_df.loc[train_df['language'] != 'English']\n",
    "without_eng_test = test_df.loc[test_df['language'] != 'English']\n",
    "\n",
    "clean_only_eng_train = clean_train_df.loc[clean_train_df['language'] == 'English']\n",
    "clean_only_eng_test = clean_test_df.loc[clean_test_df['language'] == 'English']\n",
    "clean_without_eng_train = clean_train_df.loc[clean_train_df['language'] != 'English']\n",
    "clean_without_eng_test = clean_test_df.loc[clean_test_df['language'] != 'English']\n",
    "\n",
    "only_eng_train.to_csv('only_eng_train.csv', index=False)\n",
    "only_eng_test.to_csv('only_eng_test.csv', index=False)\n",
    "without_eng_train.to_csv('without_eng_train.csv', index=False)\n",
    "without_eng_test.to_csv('without_eng_test.csv', index=False)\n",
    "\n",
    "clean_only_eng_train.to_csv('clean_only_eng_train.csv', index=False)\n",
    "clean_only_eng_test.to_csv('clean_only_eng_test.csv', index=False)\n",
    "clean_without_eng_train.to_csv('clean_without_eng_train.csv', index=False)\n",
    "clean_without_eng_test.to_csv('clean_without_eng_test.csv', index=False)\n",
    "\n",
    "clean_without_eng_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "\n",
    "punctuation = [i for i in string.punctuation]\n",
    "\n",
    "stopwords = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# 移除標點符號\n",
    "def remove_punctuation(input_string):\n",
    "    translator = str.maketrans(\"\", \"\", string.punctuation)\n",
    "    return input_string.translate(translator)\n",
    "\n",
    "# 移除 stop words\n",
    "def remove_stop_word(input_string, stop_words):\n",
    "    words = word_tokenize(input_string)\n",
    "    filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "    res = ' '.join(filtered_words)\n",
    "    \n",
    "    # 檢查結果是否為空字串，如果是，就保留原始字串\n",
    "    if res.strip() == '':\n",
    "        return input_string\n",
    "    else:\n",
    "        return res\n",
    "\n",
    "def process_str(input_string, stop_words):\n",
    "    no_punctuation = remove_punctuation(input_string)\n",
    "    return remove_stop_word(no_punctuation, stop_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_982506/2864943872.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  only_eng_train[f\"processed_{col_name}\"] = processed\n",
      "/tmp/ipykernel_982506/2864943872.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  only_eng_train[f\"processed_{col_name}\"] = processed\n",
      "/tmp/ipykernel_982506/2864943872.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  only_eng_test[f\"processed_{col}\"] = processed\n",
      "/tmp/ipykernel_982506/2864943872.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  only_eng_test[f\"processed_{col}\"] = processed\n"
     ]
    }
   ],
   "source": [
    "for col_name in [\"premise\", \"hypothesis\"]:\n",
    "    processed = []\n",
    "    for data in only_eng_train[col_name].tolist():\n",
    "        processed.append(process_str(data, stopwords))\n",
    "    only_eng_train[f\"processed_{col_name}\"] = processed\n",
    "\n",
    "for col in [\"premise\", \"hypothesis\"]:\n",
    "    processed = []\n",
    "    for data in only_eng_test[col].tolist():\n",
    "        processed.append(process_str(data, stopwords))\n",
    "    only_eng_test[f\"processed_{col}\"] = processed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_982506/2393192697.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  clean_only_eng_train[f\"processed_{col_name}\"] = processed\n",
      "/tmp/ipykernel_982506/2393192697.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  clean_only_eng_train[f\"processed_{col_name}\"] = processed\n",
      "/tmp/ipykernel_982506/2393192697.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  clean_only_eng_test[f\"processed_{col}\"] = processed\n",
      "/tmp/ipykernel_982506/2393192697.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  clean_only_eng_test[f\"processed_{col}\"] = processed\n"
     ]
    }
   ],
   "source": [
    "for col_name in [\"premise\", \"hypothesis\"]:\n",
    "    processed = []\n",
    "    for data in clean_only_eng_train[col_name].tolist():\n",
    "        processed.append(process_str(data, stopwords))\n",
    "    clean_only_eng_train[f\"processed_{col_name}\"] = processed\n",
    "\n",
    "for col in [\"premise\", \"hypothesis\"]:\n",
    "    processed = []\n",
    "    for data in clean_only_eng_test[col].tolist():\n",
    "        processed.append(process_str(data, stopwords))\n",
    "    clean_only_eng_test[f\"processed_{col}\"] = processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_982506/1191563467.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  deal_only_eng_train['premise'] = only_eng_train['processed_premise']\n",
      "/tmp/ipykernel_982506/1191563467.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  deal_only_eng_train['hypothesis'] = only_eng_train['processed_hypothesis']\n",
      "/tmp/ipykernel_982506/1191563467.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  deal_only_eng_test['premise'] = only_eng_test['processed_premise']\n",
      "/tmp/ipykernel_982506/1191563467.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  deal_only_eng_test['hypothesis'] = only_eng_test['processed_hypothesis']\n",
      "/tmp/ipykernel_982506/1191563467.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  clean_deal_only_eng_train['premise'] = clean_only_eng_train['processed_premise']\n",
      "/tmp/ipykernel_982506/1191563467.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  clean_deal_only_eng_train['hypothesis'] = clean_only_eng_train['processed_hypothesis']\n",
      "/tmp/ipykernel_982506/1191563467.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  clean_deal_only_eng_test['premise'] = clean_only_eng_test['processed_premise']\n",
      "/tmp/ipykernel_982506/1191563467.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  clean_deal_only_eng_test['hypothesis'] = clean_only_eng_test['processed_hypothesis']\n"
     ]
    }
   ],
   "source": [
    "deal_only_eng_train = only_eng_train\n",
    "deal_only_eng_train['premise'] = only_eng_train['processed_premise']\n",
    "deal_only_eng_train['hypothesis'] = only_eng_train['processed_hypothesis']\n",
    "deal_only_eng_train = only_eng_train.drop(['processed_premise', 'processed_hypothesis'], axis=1)\n",
    "\n",
    "deal_only_eng_test = only_eng_test\n",
    "deal_only_eng_test['premise'] = only_eng_test['processed_premise']\n",
    "deal_only_eng_test['hypothesis'] = only_eng_test['processed_hypothesis']\n",
    "deal_only_eng_test = only_eng_test.drop(['processed_premise', 'processed_hypothesis'], axis=1)\n",
    "\n",
    "clean_deal_only_eng_train = clean_only_eng_train\n",
    "clean_deal_only_eng_train['premise'] = clean_only_eng_train['processed_premise']\n",
    "clean_deal_only_eng_train['hypothesis'] = clean_only_eng_train['processed_hypothesis']\n",
    "clean_deal_only_eng_train = clean_only_eng_train.drop(['processed_premise', 'processed_hypothesis'], axis=1)\n",
    "\n",
    "clean_deal_only_eng_test = clean_only_eng_test\n",
    "clean_deal_only_eng_test['premise'] = clean_only_eng_test['processed_premise']\n",
    "clean_deal_only_eng_test['hypothesis'] = clean_only_eng_test['processed_hypothesis']\n",
    "clean_deal_only_eng_test = clean_only_eng_test.drop(['processed_premise', 'processed_hypothesis'], axis=1)\n",
    "\n",
    "\n",
    "deal_only_eng_train.to_csv('deal_only_eng_train.csv', index=False)\n",
    "deal_only_eng_test.to_csv('deal_only_eng_test.csv', index=False)\n",
    "\n",
    "clean_deal_only_eng_train.to_csv('clean_deal_only_eng_train.csv', index=False)\n",
    "clean_deal_only_eng_test.to_csv('clean_deal_only_eng_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, data_list, is_test=False): # data_list = [['premise', 'hypothesis', 'label']], [], ..., []\n",
    "        self.data = data_list\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "        self.is_test = is_test \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        premise = self.data[idx][0]\n",
    "        hypothesis = self.data[idx][1]\n",
    "        \n",
    "        encoding = self.tokenizer(premise, hypothesis,\n",
    "                                 add_special_tokens=True,\n",
    "                                 max_length=128,\n",
    "                                 padding='max_length',\n",
    "                                 truncation=True,\n",
    "                                 return_tensors='pt')\n",
    "        input_ids = encoding['input_ids'].squeeze()\n",
    "        attention_mask = encoding['attention_mask'].squeeze()\n",
    "        \n",
    "        if not self.is_test:\n",
    "            label = torch.tensor(self.data[idx][2], dtype=torch.long)\n",
    "            return {'input_ids': input_ids,\n",
    "                    'attention_mask': attention_mask,\n",
    "                    'label': label}\n",
    "        else:\n",
    "            return {'input_ids': input_ids,\n",
    "                    'attention_mask': attention_mask}\n",
    "\n",
    "class DataProcessor():\n",
    "    \n",
    "    def __init__(self, path):\n",
    "        self.data_df = pd.read_csv(path, encoding='ISO-8859-1')\n",
    "            \n",
    "    def get_dataloader(self):\n",
    "        # training\n",
    "        data_list = self.data_df[['premise', 'hypothesis', 'label']].values.tolist()\n",
    "        random.shuffle(data_list)\n",
    "        k = len(data_list) // 5\n",
    "        \n",
    "        train_dataset = MyDataset(data_list[k:])\n",
    "        train_dataloader = DataLoader(train_dataset, batch_size=8)\n",
    "        dev_dataset = MyDataset(data_list[:k])\n",
    "        dev_dataloader = DataLoader(dev_dataset, batch_size=16)\n",
    "        \n",
    "        return train_dataloader, dev_dataloader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3, Accuracy: 0.5448\n",
      "Epoch 2/3, Accuracy: 0.5667\n",
      "Epoch 3/3, Accuracy: 0.5562\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "# 定義模型、損失函數和優化器\n",
    "model = AutoModelForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=3)  # 有3個類別：entailment、neutral、contradiction\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "# 創建 DataProcessor 的實例\n",
    "data_processor = DataProcessor('clean_without_eng_train.csv')\n",
    "\n",
    "# 獲取資料載入器\n",
    "train_dataloader, dev_dataloader = data_processor.get_dataloader()\n",
    "\n",
    "# 訓練迴圈\n",
    "num_epochs = 3  # 你可以根據需要調整這個數字\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for batch in train_dataloader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # 驗證\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        total_correct = 0\n",
    "        total_samples = 0\n",
    "        for batch in dev_dataloader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            _, predicted = torch.max(outputs.logits, 1)\n",
    "\n",
    "            total_samples += labels.size(0)\n",
    "            total_correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = total_correct / total_samples\n",
    "    print(f'Epoch {epoch + 1}/{num_epochs}, Accuracy: {accuracy:.4f}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              id  prediction\n",
      "0     c6d58c3f69           2\n",
      "1     cefcc82292           1\n",
      "2     e98005252c           0\n",
      "3     58518c10ba           2\n",
      "4     c32b0d16df           0\n",
      "...          ...         ...\n",
      "2245  0242fd3023           0\n",
      "2246  c2e4b293bf           1\n",
      "2247  f88b84f4e1           0\n",
      "2248  5f90dd59b0           1\n",
      "2249  1f0ea92118           1\n",
      "\n",
      "[2250 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# 載入測試數據\n",
    "test_path = 'clean_without_eng_test.csv'\n",
    "test_df = pd.read_csv(test_path)\n",
    "\n",
    "# 創建 MyDataset 類的實例\n",
    "test_dataset = MyDataset(test_df[['premise', 'hypothesis']].values.tolist(), is_test=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=8)\n",
    "\n",
    "# 開始預測\n",
    "model.eval()\n",
    "predictions = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_dataloader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        _, predicted = torch.max(outputs.logits, 1)\n",
    "        predictions.extend(predicted.cpu().numpy())\n",
    "\n",
    "# 將預測結果添加到測試數據中\n",
    "test_df['prediction'] = predictions\n",
    "result_df = test_df[['id', 'prediction']]\n",
    "result_df.to_csv('predict3_2.csv', index=False)\n",
    "\n",
    "print(result_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 讀取 CSV 文件\n",
    "predict1_1 = pd.read_csv('predict1_1.csv')\n",
    "predict3_2 = pd.read_csv('predict3_2.csv')\n",
    "\n",
    "test_data = pd.read_csv('test.csv')\n",
    "\n",
    "# 合併 predict1_1 和 predict3_2\n",
    "merged_df = pd.concat([predict1_1, predict3_2])\n",
    "\n",
    "sorted = merged_df.set_index('id').loc[test_data['id']].reset_index()\n",
    "\n",
    "sorted.to_csv('YAAA.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_new",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
